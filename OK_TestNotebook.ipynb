{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.net.SocketTimeoutException: connect timed out\n",
       "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
       "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
       "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
       "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
       "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
       "\tat java.net.Socket.connect(Socket.java:589)\n",
       "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
       "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:432)\n",
       "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:527)\n",
       "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:211)\n",
       "\tat sun.net.www.http.HttpClient.New(HttpClient.java:308)\n",
       "\tat sun.net.www.http.HttpClient.New(HttpClient.java:326)\n",
       "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1169)\n",
       "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1105)\n",
       "\tat sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:989)\n",
       "\tat sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:987)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:782)\n",
       "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:986)\n",
       "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:933)\n",
       "\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:588)\n",
       "\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:394)\n",
       "\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:405)\n",
       "\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:397)\n",
       "\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)\n",
       "\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\n",
       "\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)\n",
       "\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)\n",
       "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)\n",
       "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:98)\n",
       "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\n",
       "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:397)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:193)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n",
       "org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1328)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n",
       "$line20.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:24)\n",
       "$line20.$read$$iwC$$iwC$$iwC.<init>(<console>:29)\n",
       "$line20.$read$$iwC$$iwC.<init>(<console>:31)\n",
       "$line20.$read$$iwC.<init>(<console>:33)\n",
       "$line20.$read.<init>(<console>:35)\n",
       "$line20.$read$.<init>(<console>:39)\n",
       "$line20.$read$.<clinit>(<console>)\n",
       "$line20.$eval$.<init>(<console>:7)\n",
       "$line20.$eval$.<clinit>(<console>)\n",
       "$line20.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:497)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:351)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:350)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:350)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(1 to 100).\n",
    "  filter(x => x % 2 == 0).\n",
    "  map(x => x * x).\n",
    "  take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toree",
   "language": "",
   "name": "toree"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
